{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "source": [
    "There are two important objects in tensorflow: variable and tensor.\n",
    "\n",
    "Variable is a variable as we know it. It's a wrapping class of a np array and always store some definite values (it's more versatile than a np array). It's intuitive. It has fixed, unchangeable shape and type (and is rather strict about types), and a variable value. However, it's possible to initialize shape = tf.TensorShape(None). Then shape will be inferred from the first assignment.  \n",
    "\n",
    "Tensor is all tensorflow is based on. Originally, everything in tensorflow is a graph, from model building to training and evaluating. In other words, all executions are lazy. When we do a+b, tf doesn't just add the two values, it records the addition as an operation along the graph. When we do run, it executes the graph and compute actual values. This is the case when eager execution is not on (in tf 2.0 it's by default on)\n",
    "\n",
    "A Tensor is such a latent variable. It may store definite numeric value, or store its dependencies on previous tensors (how to compute the value of this tensor given the value of previous tensors. or it may trace the dependency up to the very first variable (express this tensor value in terms of the value of the original input tensor values)). So in other words, a tensor stores an expression for computing its value.\n",
    "\n",
    "So, that's why a layer takes in a tensor and outputs a tensor. \n",
    "\n",
    "A model also takes in a tensor and outputs a tensor.\n",
    "\n",
    "model.input and model.output are both tensors\n",
    "\n",
    "Tensors are the core of a model or layer (with a few other parameters), so it's not surprising that we can just build new models out of old ones by passing a tensor into the model. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=string, numpy=b'helloworld'>\n<tf.Variable 'Variable:0' shape=(3,) dtype=int32, numpy=array([12,  3,  4], dtype=int32)>\n<tf.Variable 'Variable:0' shape=(3, 3) dtype=float32, numpy=\narray([[ 2., 23.,  2.],\n       [ 3.,  2.,  1.],\n       [ 3.,  2., 54.]], dtype=float32)>\n<tf.Variable 'Variable:0' shape=(2,) dtype=complex128, numpy=array([1.+2.j, 2.-3.j])>\n<tf.Variable 'Variable:0' shape=(3, 3) dtype=float32, numpy=\narray([[3., 3., 3.],\n       [3., 3., 3.],\n       [3., 3., 3.]], dtype=float32)>\ntf.Tensor(\n[[ 5. 26.  5.]\n [ 6.  5.  4.]\n [ 6.  5. 57.]], shape=(3, 3), dtype=float32)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(3,) dtype=int32, numpy=array([-2, -1,  0], dtype=int32)>"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "stringVar = tf.Variable('helloworld',tf.string)\n",
    "intVar = tf.Variable([12,3,4],tf.int32)\n",
    "floatVar = tf.Variable([[2,23,2],[3,2,1.0],[3,2,54]],tf.float32)\n",
    "complexVar = tf.Variable([1+2j,2-3j],tf.complex128) # here j represents mathematical i\n",
    "varFromTensor = tf.Variable(tf.constant(3.0,shape=(3,3)))\n",
    "# Obviously your tensor should be evaluatable eagerly, else you get Nones?\n",
    "print(stringVar)\n",
    "print(intVar)\n",
    "print(floatVar)\n",
    "print(complexVar)\n",
    "print(varFromTensor)\n",
    "print(floatVar+varFromTensor)\n",
    "# All operations +,-,*,/ etc returns a tensor in tensorflow\n",
    "# unless we use special variable operations\n",
    "intVar.assign([1,2,3])\n",
    "# when we use assign, shape should be the same\n",
    "floatVar.assign_add(floatVar)\n",
    "intVar.assign_sub(3*tf.ones(3,dtype=tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1. 2. 3. 4.]\n[[[[1.]\n   [2.]]\n\n  [[3.]\n   [4.]]]]\n"
     ]
    }
   ],
   "source": [
    "tensor1 = tf.constant([1,2,3,4],dtype=tf.float64)\n",
    "# for tensors with definite values, we can use tensor.numpy() to get its np values\n",
    "print(tensor1.numpy())\n",
    "tensor2 = tf.constant([1,2,3,4],dtype=tf.float64,shape=(1,2,2,1))\n",
    "# shape basically reshapes the input list or np array\n",
    "print(tensor2.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(4, shape=(), dtype=int32)\n[[8. 0. 0. 0.]\n [0. 8. 0. 0.]\n [0. 0. 8. 0.]\n [0. 0. 0. 8.]]\n"
     ]
    }
   ],
   "source": [
    "print(tf.rank(tensor2))\n",
    "# rank gives the space this tensor is in (or its # of dimensions)\n",
    "tensor3 = tf.reshape(tensor2, shape=(4,1))\n",
    "# tensor operators has mostly everything numpy has\n",
    "zeros = tf.zeros((2,3,4))\n",
    "ones = tf.ones((3,3,4))\n",
    "eye = tf.eye(4)*8\n",
    "print(eye.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[[0. 0. 0. 0.]\n  [0. 0. 0. 0.]\n  [0. 0. 0. 0.]]\n\n [[0. 0. 0. 0.]\n  [0. 0. 0. 0.]\n  [0. 0. 0. 0.]]\n\n [[1. 1. 1. 1.]\n  [1. 1. 1. 1.]\n  [1. 1. 1. 1.]]\n\n [[1. 1. 1. 1.]\n  [1. 1. 1. 1.]\n  [1. 1. 1. 1.]]\n\n [[1. 1. 1. 1.]\n  [1. 1. 1. 1.]\n  [1. 1. 1. 1.]]]\n"
     ]
    }
   ],
   "source": [
    "concate = tf.concat([zeros,ones],0)\n",
    "print(concate.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/job:localhost/replica:0/task:0/device:CPU:0\n"
     ]
    }
   ],
   "source": [
    "tf.config.list_physical_devices()\n",
    "# list all the cpus and gpus accessible\n",
    "# tensorflow automatically allocates a tensor operation to cpu or gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/job:localhost/replica:0/task:0/device:CPU:0\n"
     ]
    }
   ],
   "source": [
    "concate.device\n",
    "# check device used by a tensor through tensor.device (note we are allocating tensors to devices \n",
    "# because tensors are expressions that ultimately need to be computed, \n",
    "# so an even allocation of tensors to devices can ensure the fast speed when we finally execute the graph)\n",
    "\n",
    "# CPU:#, # means the index of the cpu or gpu used\n",
    "\n",
    "# GPUs are fast for matrix addition while CPUs are fast for matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can enforce the usage of a particular core by \n",
    "# tf.device('CPU:0') function\n",
    "with tf.device('CPU:0'):\n",
    "    a = tf.ones((3,3))\n",
    "    # using with so that this enforcement ends after the block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}