{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import reuters\n",
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list([1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12])\n",
      " list([1, 3267, 699, 3434, 2295, 56, 2, 7511, 9, 56, 3906, 1073, 81, 5, 1198, 57, 366, 737, 132, 20, 4093, 7, 2, 49, 2295, 2, 1037, 3267, 699, 3434, 8, 7, 10, 241, 16, 855, 129, 231, 783, 5, 4, 587, 2295, 2, 2, 775, 7, 48, 34, 191, 44, 35, 1795, 505, 17, 12])\n",
      " list([1, 53, 12, 284, 15, 14, 272, 26, 53, 959, 32, 818, 15, 14, 272, 26, 39, 684, 70, 11, 14, 12, 3886, 18, 180, 183, 187, 70, 11, 14, 102, 32, 11, 29, 53, 44, 704, 15, 14, 19, 758, 15, 53, 959, 47, 1013, 15, 14, 19, 132, 15, 39, 965, 32, 11, 14, 147, 72, 11, 180, 183, 187, 44, 11, 14, 102, 19, 11, 123, 186, 90, 67, 960, 4, 78, 13, 68, 467, 511, 110, 59, 89, 90, 67, 1390, 55, 2678, 92, 617, 80, 1274, 46, 905, 220, 13, 4, 346, 48, 235, 629, 5, 211, 5, 1118, 7, 2, 81, 5, 187, 11, 15, 9, 1709, 201, 5, 47, 3615, 18, 478, 4514, 5, 1118, 7, 232, 2, 71, 5, 160, 63, 11, 9, 2, 81, 5, 102, 59, 11, 17, 12])\n",
      " ...\n",
      " list([1, 141, 3890, 387, 81, 8, 16, 1629, 10, 340, 1241, 850, 31, 56, 3890, 691, 9, 1241, 71, 9, 5985, 2, 2, 699, 2, 2, 2, 699, 244, 5945, 4, 49, 8, 4, 656, 850, 33, 2993, 9, 2139, 340, 3371, 1493, 9, 2, 22, 2, 1094, 687, 83, 35, 15, 257, 6, 57, 9190, 7, 4, 5956, 654, 5, 2, 6191, 1371, 4, 49, 8, 16, 369, 646, 6, 1076, 7, 124, 407, 17, 12])\n",
      " list([1, 53, 46, 957, 26, 14, 74, 132, 26, 39, 46, 258, 3614, 18, 14, 74, 134, 5131, 18, 88, 2321, 72, 11, 14, 1842, 32, 11, 123, 383, 89, 39, 46, 235, 10, 864, 728, 5, 258, 44, 11, 15, 22, 753, 9, 42, 92, 131, 728, 5, 69, 312, 11, 15, 22, 222, 2, 3237, 383, 48, 39, 74, 235, 10, 864, 276, 5, 61, 32, 11, 15, 21, 4, 211, 5, 126, 1072, 42, 92, 131, 46, 19, 352, 11, 15, 22, 710, 220, 9, 42, 92, 131, 276, 5, 59, 61, 11, 15, 22, 10, 455, 7, 1172, 137, 336, 1325, 6, 1532, 142, 971, 6463, 43, 359, 5, 4, 326, 753, 364, 17, 12])\n",
      " list([1, 227, 2406, 91, 2, 125, 2855, 21, 4, 3976, 76, 7, 4, 757, 481, 3976, 790, 5259, 5654, 9, 111, 149, 8, 7, 10, 76, 223, 51, 4, 417, 8, 1047, 91, 6917, 1688, 340, 7, 194, 9411, 6, 1894, 21, 127, 2151, 2394, 1456, 6, 3034, 4, 329, 433, 7, 65, 87, 1127, 10, 8219, 1475, 290, 9, 21, 567, 16, 1926, 24, 4, 76, 209, 30, 4033, 6655, 5654, 8, 4, 60, 8, 4, 966, 308, 40, 2575, 129, 2, 295, 277, 1071, 9, 24, 286, 2114, 234, 222, 9, 4, 906, 3994, 8519, 114, 5758, 1752, 7, 4, 113, 17, 12])]\n"
     ]
    }
   ],
   "source": [
    "print(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = reuters.get_word_index()\n",
    "index_word = dict([(value, key) for (key, value) in word_index.items()])\n",
    "decoded_newswire = ' '.join([index_word.get(i-3, '?') for i in train_data[0]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3\n"
     ]
    }
   ],
   "source": [
    "print(decoded_newswire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences),dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        for word in sequence:\n",
    "            results[i, word]+=1.0\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(labels, dimension=46):\n",
    "    results = np.zeros((len(labels), dimension))\n",
    "    for i, label in enumerate(labels):\n",
    "        results[i,label]+=1.0\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorizedTrainLabel = to_one_hot(train_labels)\n",
    "categorizedTestLabel = to_one_hot(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def selfdef_activation(x):\n",
    "    return K.log(K.relu(x)+0.639)**2/2+0.7*K.relu(x)-0.1003\n",
    "    #f\\left(x\\right)=\\frac{\\ln\\left(\\max\\left(x,0\\right)+0.639\\right)^{2}}{2}+0.7\\max\\left(x,0\\right)-0.1003\n",
    "# Relu is a good activation function because its derivative is constant, \n",
    "# which means that the learning rate is almost exclusively governed by the weights. \n",
    "# In this way, gradient explosion and gradient vanishing problems can be alleviated. \n",
    "# Sigmoid and tanh have derivative approaching 0 when the function input approaches \n",
    "# infinity in both directions and this reduces the gradient explosion problem but may \n",
    "# cause gradient vanishing problem. Relu is not a smooth function (its derivative jumps \n",
    "# from 0 to 1 when crossing x=0 threshold). This generates inconsistency in training \n",
    "# (accuracy may jump around a little bit especially when the batch size is small, \n",
    "# in which case a smooth activation function may perform better). Because of its angularity,\n",
    "# relu is less susceptible to being trapped at local minima (enabling the use of larger batch size). \n",
    "# Itâ€™s also more similar to the biological neuron firing threshold and pattern. Smoothening the relu\n",
    "# function can theoretically increase its stability in performance. Further, reducing the derivative\n",
    "# of relu near infinity may help to suppress the gradient explosion problem and introducing a smooth\n",
    "# transition of derivative near 0 may help to reduce the dead-neuron problem. However, if done inappropriately,\n",
    "# this could increase the computation time and susceptibility to local minima. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(128, activation='relu', input_shape=(10000,)))\n",
    "# why does blowing the layer up work? Blowing the layer up exacerbates the gradient explosion problem while\n",
    "# reducing gradient vanishing problem. This is often good especially in cases where the relationship between\n",
    "# features and their effects seems direct (that is, a feature directly contributes to the probability of classifying\n",
    "# to a specific group. A feature is not much related to other features where they must satisfy some condition to\n",
    "# contribute to classifying to a specific group)\n",
    "model.add(layers.Dense(256, activation='relu', input_shape=(128,)))\n",
    "# at any time, the number of nodes in a layer should be greater than the number of output nodes. This is apparent if \n",
    "# we view each layer as a representation of the input info provided. A representation more compressed than desired may\n",
    "# lose us information.\n",
    "model.add(layers.Dense(46, activation='softmax', input_shape=(256,)))\n",
    "# softmax activation function is an activation function applied to the entire layer where it\n",
    "# calculates the probability of each of the nodes being the correctly activated (as in a classification problem)\n",
    "# node. It doesn't just divide a number by the total, it applies exponential function to the values\n",
    "# before dividing each by the total. This makes sense because exponential is often internally related to\n",
    "# probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train[:1000]\n",
    "partial_x_train = x_train[1000:]\n",
    "\n",
    "y_val = categorizedTrainLabel[:1000]\n",
    "partial_y_train = categorizedTrainLabel[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 1.3132 - accuracy: 0.7256 - val_loss: 0.9357 - val_accuracy: 0.8020\n",
      "Epoch 2/5\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 0.6209 - accuracy: 0.8642 - val_loss: 0.8653 - val_accuracy: 0.8340\n",
      "Epoch 3/5\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 0.3824 - accuracy: 0.9129 - val_loss: 0.8849 - val_accuracy: 0.8360\n",
      "Epoch 4/5\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 0.2860 - accuracy: 0.9359 - val_loss: 0.9255 - val_accuracy: 0.8320\n",
      "Epoch 5/5\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 0.2282 - accuracy: 0.9469 - val_loss: 1.0472 - val_accuracy: 0.8280\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(partial_x_train,partial_y_train,epochs=5,batch_size = 32, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
