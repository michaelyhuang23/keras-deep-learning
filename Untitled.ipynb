{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_66 (Conv2D)           (None, 26, 26, 16)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_44 (MaxPooling (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_67 (Conv2D)           (None, 11, 11, 48)        6960      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_45 (MaxPooling (None, 5, 5, 48)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_68 (Conv2D)           (None, 3, 3, 64)          27712     \n",
      "=================================================================\n",
      "Total params: 34,832\n",
      "Trainable params: 34,832\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(16,(3,3),activation='relu',input_shape=(28,28,1)))\n",
    "# mathematically this is a dimensional expansion here from 3*3 to 32\n",
    "\n",
    "# as illustrated, adding number of features/filters in the earlier layers doesn't add much to the model's \n",
    "# susceptibility to overfitting, but it adds dramatically to training time. It doesn't provide a huge boost to the\n",
    "# model's performance because there are really not so many preliminary features to be learned in the first few layers\n",
    "# especially given that the previous layer has size only 3*3*1. So theoretically a feature size of 9 is sufficient to\n",
    "# retain all information. Having too many features will cause a huge dimensional reduction in the next layer which \n",
    "# could lose information However, it's also important to note that the point of such filter conv2d layer is to build\n",
    "# correlation between the inputs instead of transforming the inputs individually. There're 255^9 ways the input can \n",
    "# correlate to each other and not all of them are meaningful or even possible. Plus some of those correlations can \n",
    "# be expressed as a percentage of multiple different correlations at work together. So the number of features really\n",
    "# depends on what featuers we intuitively want it to learn at this stage. Intuitively at the first stage, we want it \n",
    "# to recognize the direction of lines, edges between black and white and their direction. Very roughly there're 8 \n",
    "# directions. Perhaps we also want to recognize features like angles or anything with two edges and that at least add\n",
    "# 8 to our count.\n",
    "\n",
    "# at first glance, it seems a sigmoid activation function makes sense since we are classifying whether a feature exist\n",
    "# or not. However, it in fact cannot and shouldn't be classified as either exist or not. Instead, for each feature we\n",
    "# we should indicate how similar is this pattern in the filter window is to that feature, for which case a relu makes\n",
    "# more sense. Additionally and unsurprisingly, a relu can be trained more easily.\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(48,(3,3),activation='relu'))\n",
    "# this goes from 3*3*32 to 64\n",
    "\n",
    "# we might want to see lines and turning points and circles in this one: roughly 8 neurons for lines, 32 for turning\n",
    "# points and just add another 8 for other things.\n",
    "\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(64,(3,3),activation='relu'))\n",
    "# this goes from 3*3*64 to 64\n",
    "\n",
    "# looking at the resolution of the input: 5*5, it looks like we need to recognize things like circles, curves and \n",
    "# other abstract high-level features of numbers: turning points, intersections, end of a line (curly end and straight\n",
    "# end), directional half circle, moon shape, etc. Let's give 32 for turning points, 4 for intersections, 12 for end\n",
    "# of a line, 8 for directional half circles, 4 for moon shapes and 4 for circles (we give more than needed to make the\n",
    "# model less susceptible to scaling and translation). \n",
    "\n",
    "# for each filtering of features, since we are only applying one linear transformation followed by curbings, we must\n",
    "# consider if this is enough transformation to derive the features from the older ones, if not we might want to either\n",
    "# apply two conv2d layers consecutively or enlarge the transformation at each conv2d layer to include some hidden\n",
    "# layers. The first method has a richer hypothesis space while the second has more regularity and is thus less prone\n",
    "# to overfitting.\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "# model.add(layers.Flatten())\n",
    "# model.add(layers.Dense(10, activation='sigmoid'))\n",
    "# these two alone have the same mathematical effect of a new conv2d layer attached at the end\n",
    "# adding an intermediate layer expands the hypothesis space and reduces the info lost during the huge dimensional\n",
    "# reduction from 3*3*64 to 10 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.14117648 0.57254905 0.99607843 1.0 0.9843137 0.37254903 0.023529412 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.011764706 0.38039216 0.91764706 0.99607843 0.99607843 0.9098039 0.99607843 0.99607843 0.13725491 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.34901962 0.54901963 0.99607843 0.99607843 0.68235296 0.2627451 0.12941177 0.78431374 0.99607843 0.74509805 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.42352942 0.99215686 0.99607843 0.92156863 0.2 0.003921569 0.0 0.0 0.047058824 0.99607843 0.99215686 0.21960784 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.047058824 0.84705883 0.99607843 0.95686275 0.21568628 0.0 0.0 0.0 0.0 0.023529412 0.8352941 0.99607843 0.22352941 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.09803922 0.99607843 0.99607843 0.5176471 0.0 0.0 0.0 0.0 0.0 0.0 0.65882355 0.99607843 0.22352941 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.1764706 0.99607843 0.9529412 0.13333334 0.0 0.0 0.0 0.0 0.0 0.0 0.65882355 0.99607843 0.22352941 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5019608 0.99607843 0.6156863 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.65882355 0.99607843 0.22352941 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.07450981 0.89411765 0.99607843 0.4117647 0.0 0.0 0.0 0.0 0.0 0.0 0.02745098 0.89411765 0.99607843 0.22352941 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.22745098 0.99607843 0.99607843 0.34117648 0.0 0.0 0.0 0.0 0.0 0.0 0.039215688 0.99607843 0.9647059 0.18431373 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.22745098 0.99607843 0.99607843 0.03529412 0.0 0.0 0.0 0.0 0.0 0.0 0.039215688 0.99607843 0.8235294 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.22745098 0.99607843 0.99607843 0.03529412 0.0 0.0 0.0 0.0 0.0 0.0 0.4117647 0.99607843 0.35686275 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.019607844 0.85882354 0.99607843 0.03529412 0.0 0.0 0.0 0.0 0.0 0.09411765 0.9019608 0.99607843 0.09411765 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.84705883 0.99607843 0.03529412 0.0 0.0 0.0 0.0 0.0 0.32941177 0.99607843 0.9843137 0.09019608 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.84705883 0.99607843 0.14117648 0.0 0.0 0.0 0.0 0.08627451 0.8156863 0.9843137 0.36862746 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5058824 0.99607843 0.47058824 0.0 0.0 0.0 0.011764706 0.54901963 0.99607843 0.8980392 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.3254902 0.99607843 0.87058824 0.06666667 0.0 0.0 0.35686275 0.99607843 0.9254902 0.20784314 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.07058824 0.92156863 0.99607843 0.5254902 0.08235294 0.46666667 0.92941177 0.99607843 0.4862745 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.20784314 0.9764706 0.99607843 0.91764706 0.9882353 0.99607843 0.6745098 0.011764706 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.45490196 0.92941177 0.99607843 0.99607843 0.52156866 0.078431375 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "(train_images, train_labels),(test_images, test_labels) = mnist.load_data()\n",
    "train_images = train_images.reshape(np.append(np.shape(train_images),(1)))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape(np.append(np.shape(test_images),(1)))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "partial_train_images = train_images[1000:2000]\n",
    "partial_train_labels = train_labels[1000:2000]\n",
    "\n",
    "val_images = train_images[:1000]\n",
    "val_labels = train_labels[:1000]\n",
    "for row in partial_train_images[0]:\n",
    "    print(' '.join([str(i[0]) for i in row]))\n",
    "#print(partial_train_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.9063 - accuracy: 0.7030 - val_loss: 0.4039 - val_accuracy: 0.8830\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.2971 - accuracy: 0.9050 - val_loss: 0.3297 - val_accuracy: 0.9100\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.2161 - accuracy: 0.9370 - val_loss: 0.3602 - val_accuracy: 0.9080\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1423 - accuracy: 0.9600 - val_loss: 0.2747 - val_accuracy: 0.9340\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1136 - accuracy: 0.9750 - val_loss: 0.3235 - val_accuracy: 0.9450\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0694 - accuracy: 0.9810 - val_loss: 0.4297 - val_accuracy: 0.9340\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0485 - accuracy: 0.9900 - val_loss: 0.4429 - val_accuracy: 0.9400\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0759 - accuracy: 0.9820 - val_loss: 0.5406 - val_accuracy: 0.9320\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0182 - accuracy: 0.9930 - val_loss: 0.5613 - val_accuracy: 0.9380\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0428 - accuracy: 0.9930 - val_loss: 0.5461 - val_accuracy: 0.9450\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0322 - accuracy: 0.9950 - val_loss: 0.7338 - val_accuracy: 0.9450\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0281 - accuracy: 0.9930 - val_loss: 0.6137 - val_accuracy: 0.9430\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0125 - accuracy: 0.9970 - val_loss: 0.6372 - val_accuracy: 0.9490\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0063 - accuracy: 0.9990 - val_loss: 0.8358 - val_accuracy: 0.9460\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0256 - accuracy: 0.9970 - val_loss: 0.8170 - val_accuracy: 0.9450\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0057 - accuracy: 0.9980 - val_loss: 0.9203 - val_accuracy: 0.9430\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0104 - accuracy: 0.9970 - val_loss: 0.8751 - val_accuracy: 0.9520\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0122 - accuracy: 0.9980 - val_loss: 0.9343 - val_accuracy: 0.9470\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0094 - accuracy: 0.9990 - val_loss: 0.8217 - val_accuracy: 0.9490\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0040 - accuracy: 0.9990 - val_loss: 0.8624 - val_accuracy: 0.9470\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 1.1157e-05 - accuracy: 1.0000 - val_loss: 1.1078 - val_accuracy: 0.9400\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0141 - accuracy: 0.9980 - val_loss: 1.0334 - val_accuracy: 0.9510\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 1.0523 - val_accuracy: 0.9530\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 1.0548 - val_accuracy: 0.9530\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 1.0569 - val_accuracy: 0.9530\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 1.0585 - val_accuracy: 0.9530\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 1.0600 - val_accuracy: 0.9530\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 1.0613 - val_accuracy: 0.9540\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 1.0625 - val_accuracy: 0.9540\n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 1.0636 - val_accuracy: 0.9540\n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 1.0647 - val_accuracy: 0.9540\n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 1.0656 - val_accuracy: 0.9540\n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 1.0666 - val_accuracy: 0.9540\n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 1.0674 - val_accuracy: 0.9540\n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 1.0683 - val_accuracy: 0.9540\n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 1.0691 - val_accuracy: 0.9540\n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 1.0698 - val_accuracy: 0.9540\n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 1.0705 - val_accuracy: 0.9540\n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 1.0713 - val_accuracy: 0.9540\n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 1.0719 - val_accuracy: 0.9540\n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 1.0726 - val_accuracy: 0.9540\n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 1.0732 - val_accuracy: 0.9540\n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 1.0739 - val_accuracy: 0.9550\n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 1.0745 - val_accuracy: 0.9550\n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 1.0750 - val_accuracy: 0.9550\n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 1.0756 - val_accuracy: 0.9550\n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 1.0762 - val_accuracy: 0.9550\n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 1.0767 - val_accuracy: 0.9550\n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 1.0773 - val_accuracy: 0.9550\n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 1.0778 - val_accuracy: 0.9550\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "hist = model.fit(partial_train_images,partial_train_labels, epochs=50, batch_size=1, validation_data=(val_images, val_labels))\n",
    "# Very apparently, convolutionary neural network is not very susceptible to overfitting, in fact the majority of the \n",
    "# overfitting occurs at the end dense layers. The reason why CNN is not susceptible to overfitting is because its\n",
    "# training is intrinsically mathematically equivalent to data augmentation. The same filtering window is trained on\n",
    "# every possible slots where we can fit a window, which is mathematically equivalent to having a lot of training \n",
    "# samples. This is especially true for earlier layers. It also makes intuitive sense since earlier layers are learning\n",
    "# basic, common and thus non-specific features such as lines, curves, circles and etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.9868 - accuracy: 0.9561\n",
      "loss:  0.9867855310440063   accuracy:  0.9560999870300293\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print('loss: ',test_loss,'  accuracy: ',test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc181cbf2d0>]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAqsklEQVR4nO3deXSV9b3v8fc3I4QpQMIgQSYZRUClVNQ6thY6OdS22sGWDpZT9diu3tVa7+14Ts+xd5321N56yrFVa1tbax2q9eBUquKEGBACCAiCShgzkIQEkp3he//YT+LOZifZwn4SsvfntRaLPNN+vk8t+5vf7/f8vj9zd0REROJl9XUAIiJyYlKCEBGRhJQgREQkISUIERFJSAlCREQSyunrAFKpqKjIJ06c2NdhiIj0G2vWrKl09+JEx9IqQUycOJHS0tK+DkNEpN8ws7e6OqYuJhERSUgJQkREElKCEBGRhJQgREQkISUIERFJSAlCREQSUoIQEZGE0moehIjI8ao41MTD63ZTd6S5r0NJWkF+DkvPn5Lyz1WCEBEBtuyr447ndvLwuj1EWtsw6+uIklc0OF8JQkQkldranGdeP8Adz+/khe1VDMzN5lPvGc+ScyYyuXhwX4fX55QgRE5Qjc2t/PXV3Txatpemlta+Dict7a9r4u3qw4wZOoBvL5rB1QvGU1iQ19dhnTCUIEROMAcONfKHl97iDy+/TXVDhFNGDWbUkPy+DistnTJqMN+8ZBofOm0sudl6ZyeeEoRIElrbnLDXb399fz13PL+Tv63fQ3NbGxfPGMWXzp3MWZNHYP2pQ1zShhKESA/ufH4ntzy2hUhrW+j3GpibzVULxrPknElMKhoU+v1EuqMEIdKFtjbn35Zv5jfP7+SC6cWcefLwUO9XWJDLx+aOY1hBbqj3EUmWEoScUOqbWqg5HKFkeEGfxtHY3Mo3/7Ke/ynbyxfOnsh3PzKL7Cx180hm0aiMnFB+8tgWFt/6HLV9OEmp9nAz19y5mv8p28vNH5rB9z+q5CCZSQlCTijPbavgUGML965+u0/uv7vmCB9f9iLr3q7hF1efzrXnTdEAsWQsJQg5YeytPcKbVYfJyTLueuFNIi3hDwrHeqOinstve4H9dY3c/cUFfGzuSb16f5ETTagJwswWmdlWM9tuZjclOD7czB4yszIzW21ms2OOvWlmG8xsnZlpoekMsGpHFQDf+MA09tU18mjZnl69/53P76S+qYX7l57Nwikje/XeIiei0BKEmWUDtwGLgVnA1WY2K+60m4F17j4HuAa4Ne74he4+z93nhxWnnDhWvVHNsIG5LD1/ClNHDeb2lTtCn3sQq6y8lnnjC5k+Zkiv3VPkRBZmC2IBsN3dd7h7BLgXuDTunFnACgB33wJMNLPRIcYkJ7BVO6tYMGkE2VnGV86bzJZ9h3h+e2Wv3LuppZUt++qYU1LYK/cT6Q/CTBDjgF0x2+XBvljrgSsAzGwBMAEoCY458KSZrTGza7u6iZlda2alZlZaUVGRsuCld+2pOcJbVYdZODnatXPpvJMoHpLP7St39Mr9t+w9RHOrM7dkWK/cT6Q/CDNBJHr1I76/4BZguJmtA24AXgVagmPnuPsZRLuorjOz8xLdxN1vd/f57j6/uLg4NZFLl8Lq8mkffzgrSBD5Odl84eyJPLetks1760KPqay8BoDTlCBEOoSZIMqB8THbJUCnUUd3r3P3Je4+j+gYRDGwMzi2J/j7APAQ0S4r6UOPb9zHuT95mu0H6lP+2at2VFFYkMuMmP7/z7z3ZArysvnNczu7vO6xDXuZ+8Mn+cWKbceVKMrKaxk5KI9xhQOP+TNE0k2YCeIVYKqZTTKzPOAq4JHYE8ysMDgG8GVgpbvXmdkgMxsSnDMIuATYGGKskoRfP7eD3TVHuO6etRyJpLb89Es7qnjvpBFkxUxIKyzI45Pzx/PI+t3sq2086po7n9/J1/64ltzsLH721Ot858ENtBxjvaSy8lrmlAzTnAeRGKElCHdvAa4HngA2A/e5+yYzW2pmS4PTZgKbzGwL0a6kG4P9o4HnzWw9sBr4H3d/PKxYpWc7KupZ89ZB3j9zFK8fOMT3H0ldvi4/eJhd1Uc6updifencSbS2Ob998c2OfW1tzr8++ho/evQ1Lpk1mue/fRE3XHQK976yi6/8rpSGppajPqc7hyMtbDtwiNM0QC3SSai1mNx9ObA8bt+ymJ9fAqYmuG4HMDfM2OTduX9NOdlZxr9dfhq/X/UW/+8f21kwaSRXnlnS88U9eHlHNUDCuQfjRxSw+LSx3PPyW1x/0SnkZFlHjaTPL5zA9z56KtlZxjcvmc6YYQP47l83cvWvV3HH599DcZJrKGzaU0ebowFqkTiaSS09am1zHly7m/OnFTNq6AC+/v5pnDV5BP/nrxt4ff+h4/78l3ZUMbwgl2mjEs8/uPZ9kznU2MLtK3d01Ej6zuIZ/OBjp3aqkfSZ907g19fMZ9v+eq741QvsqEhurGT9rhpAA9Qi8ZQgpEfPb69kX10jnwhaC9lZxi+uOp3B+bl87Z6177pLJ96qHVW8d9LITuMPseaOL2TBpBH8YsU2Xn37ILdeNY+vnp+4RtLFM0fzp2vP4nBTKx//1Yts3F3b4/3LymsZO2wAo4YMOK7nEEk3ShDSo7+U7qKwIJeLZo7q2Ddq6ABuvWoeb1TU892/bjzmN4h2VR+m/OCRHktbfOP905hSPIi7v7iAS+fFT6fpbN74Qh782tlkZxk///u2HmPYsDs6QC0inSlBSLdqDzfz5Gv7uWzeOPJzsjsdO+eUIm68eCoPvrqb+0p3dfEJ3Yuf/9CVhVNGsuKbF3D2lKKkPnfCyEF8dO5JrNxWQX03LZzaI83srGzQDGqRBJQgpFuPlO0h0tLW5WD0DRdN5dxTivjew5v49cod1DW+u3UcVu2oZsSgPKaOGpyKcDtZPHsskZY2nt5yoMtzNpRHu6DUghA5mhKEdOv+NeXMGDOEU08amvB4dpbxn5+ax+knF/Lj5ZtZ+G8r+MEjm3i76nCPn+3urNpRxVmTR3Q5/nA8zpwwnKLBeTy+cV+X55TtrgFgzrjClN9fpL9TgsgAkZY26ptaEv7pbuxg2/5DrN9Vw5VnlnQ7gax4SD73XruQv11/Lh+YNZo/rHqL8//jab76+1JW76zu8h7lB4+wuybx/IdUyM4yLjl1DE9vPUBjc+KJfWW7apkwskDrQIskoDWpe0HN4QgX/fRZfvbJuVwwfVTPF6TQocZmzvu/T3PwcOKun4WTR7Lss2cm/IK8f005OVnGZad3Pyjc7rSSYfz8qtO5afFMfvfSm/xx9ds8sWk/l58+jp98fA55OZ1/H3kpyfGH47F49hj++PLbrHy9gktOHXPU8Q27azljwvDQ7i/SnylB9IK1bx+kuiHCM1srej1B/GPLAQ4ebuar50+maFDniWN1jc0se/YNrlz2Ir/94oJOdYhaWtt48NXdXDhjFEWDk5tw1m7MsAF8a9EMbrhoKsuefYNbV2zjwKFGfvXZMxk64J1EtOqNKkaGNP7Q7qzJIxk2MJfHN+47KkFU1jexu+YIS86ZGNr9RfozJYhesH5XdCB0fVAxtDc9tmEfo4fm8+0PzkjYz79wyki++vs1XH7bC9y15D2celJ0sHbltgoqDjV1zH04FgPzsvnGB6YxYWQB37q/jE8ue4m7lryHscMGxow/jAy1/lFudhbvnzmaJ1/bR6SlrVMrpqOC6zgNUIskojGIXrAhmKz12p46mo+xmNyxOBxp4ZnXD/DBU8d0OQh89pQi7l8anTPwqf9exXPbomtq/KW0nJGD8rhwxvG3eK44o4S7lryH8oNHuOK/XmTrvkPsqj7CntpGzuqFpT0Xzx7DocYWXnyj8+JDZeW1ZBnMVoIQSUgJImTuTll5DUMH5NDU0paS0hTJenZrBY3NbSyafXTfe6zpY4bw4NfOpmT4QJbc9Qp3Pr+Tv2/ez2WnjyM3OzX/F3nf1GLu++pC2ty5ctmL3Pb0dgAWTh6Rks/vzrlTixiUl80Tmzq/zVRWXsspowYzKF8NaZFElCBCtqe2kcr6CJ+YH10ao6y859IPqfL4pn0ML8hlwcSev4THDhvIfUsXsmDSCH706Gs0t3pKCvHFmnXSUB782jmMGTqAP5fuomhwPlOKwxt/aDcgN5uLZo7myU37aW2LvlHVnrhP0+utIl1SggjZhqCf+yNzxjJ0QE6vJYimllb+sfkAl8waQ06SrYChA3L57ZIFXL3gZD48Zywzxyae+3A8xhUO5P6lZ3PJrNF8esH4Xlt/YdGpY6hqiLB6Z7Ry7N4gcc8dr+4lka6obR2y9eW15GQZM8cOZU5JYcfAaNhe2F7JoaYWFp3WffdSvLycLP79itNCiipqWEEut18zP9R7xLtgejH5OVk8vnEvC6eM7PjvoBIbIl1TCyJkZeU1zBg7hAG52cwpGcbWfYe6nLSVSo9t2MeQ/BzOSbJ2UboblJ/D+dOKeWLTftravCNxxy5xKiKdKUGEqK3Ng6UsC4Hob6stbc7mvXWh3reltY2nNu/n4pmjjpqclskWnzaGfXWNrCuvYUN5bUfiFpHE9O0RoreqD3OosYU5wWuU7QXhwh6HeHlnNTWHm1k0e2yo9+lvLpoxmtxs47ENezVALZKEUBOEmS0ys61mtt3MbkpwfLiZPWRmZWa22sxmxx3PNrNXzezRMOMMS3w/99hhAyganB96gnhs414G5mZz/rTiUO/T3wwbmMvZU4q495Vd1DW2aIlRkR6EliDMLBu4DVgMzAKuNrNZcafdDKxz9znANcCtccdvBDaHFWPY1u+qJT8ni2mjo69ymhlzSoaFOlDd1uY8sWk/F84oZmCeuk/itU+aAw1Qi/QkzBbEAmC7u+9w9whwL3Bp3DmzgBUA7r4FmGhmowHMrAT4MPCbEGMM1YbdNZx60tBOr5nOKRnG9or6bhexAThQ18iu6p5LZsdb+/ZBKg41qXupCx+YNZosg/ycLKaODn8Ohkh/FmaCGAfELjNWHuyLtR64AsDMFgATgPbZWT8HvgV0W5vCzK41s1IzK62oqEhB2KnR0trGxt11R/2WOrekEHfY1MNayV+7Zy3X3Ln6XS/l+djGfeRlZ3HhdHUvJTJycD7nTytmwaQRKZslLpKuwvwXkmgGVPy33S3AcDNbB9wAvAq0mNlHgAPuvqanm7j77e4+393nFxefOF+K2yvqOdLcetRErNOSGKh+o6Ke0rcOsrOygTeTWHinnbvz+MZ9vG9qEUMGaH2DrvzXZ87k9s/17jwMkf4ozARRDoyP2S4B9sSe4O517r7E3ecRHYMoBnYC5wAfM7M3iXZNXWRmfwgx1pRrTwDxb8oUDc5nXOHAbiu73r+mnPYJxs9s7Xq5zHgbdteyu+ZIj7WXMt3AvGyNz4gkIcwE8Qow1cwmmVkecBXwSOwJZlYYHAP4MrAySBrfcfcSd58YXPcPd/9siLGmXFl5DUPyc5hcNOioY3NKhnVUeI3X2uY8uLacC6ePYnLRIJ59Pflus8c37iM7y/jArNHHHLeISLvQEoS7twDXA08QfRPpPnffZGZLzWxpcNpMYJOZbSH6ttONYcXT28rKa5k9bljCMtunlQzjrarD1ByOHHXsuW0V7K9r4sozSzhvWjGrdlQlNfO6vXtp4eSRFBbk9Xi+iEhPQh2lc/fl7j7N3ae4+4+DfcvcfVnw80vuPtXdZ7j7Fe5+MMFnPOPuHwkzzlRramll8966jolx8eYGA9eJWhH3rymnsCCXi2eO4vzpxTQ2t/FyUGCuO1v3H2JHZYO6l0QkZfQaRwi27jtEc6t3+Z59+wI18QPVtYebefK1/Vw2bxz5OdksnDyS/Jwsnt3aczfTQ2t3k5NlShAikjJKECFYH3zxd9WCGDYwl0lFg1i/q6bT/kfK9hBpaetYh2FAbjbvnTySZ1/vfqD6eNaPFhHpihJECDaU1zC8IJeS4QO7PCfRQPX9pbuYMWYIp570zjoM508r5o2Khm4nzaVi/WgRkXhKECFor+Da3WI4c0oK2VvbyIFDjQC8vv8Q68trufLMkk7XXRBMeOvubaZUrh8tItJOCSLFDkdaeH3/oS67l9p1VHbdFW1F3L+mnJws47LTO082n1w0iJLhA3mmi3GIgw2RlK8fLSICShAp99qeOtq850Jwp540lCyDst210TGEtYnHEMyM86cV8+IblURajq468vC63aGsHy0iogSRYj0NULcryMth2ughlJXX8OzrFVTWdz2GcMH0URyOtFL61tGvu96/tpzZ44aGsn60iGQ2JYgUKyuvYfTQfEYPHdDjuaeNG0ZZeW2PYwgLp4wkN9uOet118946Nu6u48oz1HoQkdRTgkix2CVGezJnfCHVDRGefG1ft2MIg/NzeM/EEUcNVP+ltJzcbOPSefFFckVEjp8SRArVHmlmZ2VD0iuVtZ/X5vQ4hnD+tGK27DvE3tojAERa2vjrut28f+Zohg9SaQ0RST0liBRqX+PhtCRbENPHDCE325IaQzg/eN11ZdCKeHrrAaobInxivrqXRCQcShApVH4w+tt9ogquieTnZPO9j57Kdz8cvxLr0aaPHsKYoQM6upnuX1NO8ZB8zpt64qyBISLpJaevA0gnlQ1NAIwcnHyXz+fOmpDUee2vuy7fuJf9dY08veUAXzp3UqflTEVEUknfLilUVR+hIC+bgrxw8u7504s51NjC9x7eSEub5j6ISLiUIFKoqr7pXbUe3q1zTikiO8t4YtN+5o4vZOroIaHdS0RECSKFKusjjBwUXjXVYQNzOePkQqDnt55ERI6XEkQKVdY3hV5ue/HssQwdkMPH5pwU6n1ERDRInUJVDRHmjS8M9R5fOHsin3rPeAbl6z+diIRLLYgUaWtzqhsioY5BAGRlmZKDiPSKUBOEmS0ys61mtt3MbkpwfLiZPWRmZWa22sxmB/sHBNvrzWyTmf0wzDhToeZIM61trhXdRCRthJYgzCwbuA1YDMwCrjaz+BlhNwPr3H0OcA1wa7C/CbjI3ecC84BFZnZWWLGmQlV9+xwIJQgRSQ9htiAWANvdfYe7R4B7gUvjzpkFrABw9y3ARDMb7VH1wTm5wR8PMdbjVlkfAaBIdZFEJE2EmSDGAbtitsuDfbHWA1cAmNkCYAJQEmxnm9k64ADwlLu/nOgmZnatmZWaWWlFRdfLcoatUi0IEUkzYSaIRAsyx7cCbgGGB4ngBuBVoAXA3VvdfR7RhLGgfXziqA90v93d57v7/OLivqtL1N7FVBTyILWISG8J83WYcmB8zHYJsCf2BHevA5YAmJkBO4M/sefUmNkzwCJgY4jxHpeqhghZBoUFShAikh7CbEG8Akw1s0lmlgdcBTwSe4KZFQbHAL4MrHT3OjMrNrPC4JyBwPuBLSHGetwq6yOMGJRHdlaihpOISP8TWgvC3VvM7HrgCSAbuNPdN5nZ0uD4MmAm8DszawVeA74UXD4WuDt4EyoLuM/dHw0r1lSorG8KtcyGiEhvC3XGlbsvB5bH7VsW8/NLwNQE15UBp4cZW6pV1TdRNETdSyKSPjSTOkWqGsIt1Cci0tuUIFKkqj78MhsiIr1JCSIFGptbqW9qUZkNEUkrShApUKk5ECKShpJKEGb2gJl92MyUUBKoCspsaAxCRNJJsl/4vwI+DWwzs1vMbEaIMfU775TZUAtCRNJHUgnC3f/u7p8BzgDeBJ4ysxfNbImZ5YYZYH/Q3oLQGISIpJOku4zMbCTwBaIznl8lWpr7DOCpUCLrRyob1IIQkfST1EQ5M3sQmAH8Hviou+8NDv3ZzErDCq6/qKqPUJCXTUGeVnoTkfSR7DfaL939H4kOuPv8FMbTL1XWN6n1ICJpJ9kuppntxfOgY6nQr4UTUv9TVR/R+IOIpJ1kE8RX3L2mfcPdDwJfCSWifkiF+kQkHSWbILKC9RqAjvWm1acSqGqIaJKciKSdZMcgngDuM7NlRFeFWwo8HlpU/Uhbm1PdoC4mEUk/ySaIbwNfBf6J6FKiTwK/CSuo/qTmSDOtba5BahFJO0klCHdvIzqb+lfhhtP/VHXMolYLQkTSS7LzIKYC/w7MAga073f3ySHF1W9UtBfqG6QWhIikl2QHqe8i2npoAS4Efkd00lzG6yizMUQtCBFJL8kmiIHuvgIwd3/L3X8AXNTTRWa2yMy2mtl2M7spwfHhZvaQmZWZ2Wozmx3sH29mT5vZZjPbZGY3vpuH6k0dXUxqQYhImkk2QTQGpb63mdn1ZnY5MKq7C4JXYW8DFhPtmrrazGbFnXYzsM7d5wDXEK3vBNGWyjfdfSZwFnBdgmtPCFUNEbIMCguUIEQkvSSbIL4OFAD/DJwJfBb4fA/XLAC2u/sOd48A9wKXxp0zC1gB4O5bgIlmNtrd97r72mD/IWAzMC7JWHtVZX0TIwblkZ1lPZ8sItKP9JgggpbAJ9293t3L3X2Ju3/c3Vf1cOk4YFfMdjlHf8mvB64I7rMAmACUxN1/InA68HIX8V1rZqVmVlpRUdHT46RcpcpsiEia6jFBuHsrcGbsTOokJTrf47ZvAYab2TrgBqJlxFs6PsBsMPAA8HV3r+sivtvdfb67zy8uLn6XIR6/KhXqE5E0lexEuVeBh83sL0BD+053f7Cba8qB8THbJcCe2BOCL/0lAEEC2hn8IViI6AHgnh7u06eqGiLMHV7Y12GIiKRcsgliBFBF5zeXHOjui/sVYKqZTQJ2A1cRXba0Q1Ah9nAwRvFlYKW71wXJ4g5gs7v/LMkY+0TloSZ1MYlIWkp2JvWSd/vB7t5iZtcTreOUDdzp7pvMbGlwfBkwE/idmbUCrwFfCi4/B/gcsCHofgK42d2Xv9s4wnQk0kpDpFVdTCKSlpKdSX0XR48f4O5f7O664At9edy+ZTE/vwRMTXDd8yQewzihVAVLjaqSq4iko2S7mB6N+XkAcDlx4wmZqDKYRa21IEQkHSXbxfRA7LaZ/Qn4eygR9SPts6hVZkNE0lGyE+XiTQVOTmUg/VFVRwtCXUwikn6SHYM4ROcxiH1E14jIaJUdYxBqQYhI+km2i2lI2IH0R5WHIgzKy2ZgXnZfhyIiknJJdTGZ2eVmNixmu9DMLgstqn6iqqFJCwWJSNpKdgzi++5e277h7jXA90OJqB+pqo9oDoSIpK1kE0Si85J9RTZtVdY36RVXEUlbySaIUjP7mZlNMbPJZvafwJowA+sPKusjFA9RC0JE0lOyCeIGIAL8GbgPOAJcF1ZQ/UFbm1PdoBaEiKSvZN9iagCOWjI0k9UcaabN0RiEiKStZN9ieiqovNq+PdzMnggtqn6gsn0tar3FJCJpKtkupqLgzSUA3P0gPaxJne7aE4QK9YlIuko2QbSZWUdpjWAZ0KOqu2aS9jIbmkUtIukq2VdV/zfwvJk9G2yfB1wbTkj9Q3uhPtVhEpF0lewg9eNmNp9oUlgHPEz0TaaMVVkfIctgeIEShIikp2SL9X0ZuJHoutLrgLOAl+i8BGlGqWpoYsSgfLKyTvh1jUREjkmyYxA3Au8B3nL3C4HTgYrQouoHKusjGqAWkbSWbIJodPdGADPLd/ctwPTwwjrxVdU3aQ6EiKS1ZBNEeTAP4q/AU2b2MEksOWpmi8xsq5ltN7OjJtoF8ykeMrMyM1ttZrNjjt1pZgfMbGOSMfaqaAtCbzCJSPpKKkG4++XuXuPuPwC+C9wBXNbdNWaWDdwGLAZmAVeb2ay4024G1rn7HOAa4NaYY78FFiUTX1+oUqE+EUlz73rJUXd/1t0fcfdID6cuALa7+47g3HuBS+POmQWsCD53CzDRzEYH2yuB6ncbXyr98G+b+MJdq6k90txp/5FIKw2RVnUxiUhaO9Y1qZMxDtgVs10e7Iu1HrgCwMwWABOIvimVNDO71sxKzay0oiK14+Yv76jmma0VfGLZi+ypeeetXs2iFpFMEGaCSPT+Z/zs61uA4Wa2jmjF2FeBlndzE3e/3d3nu/v84uLiYwq0K1UNTZw2bhh7axq5/L9eYPPeumC/ZlGLSPoLM0GUA+NjtkuIG9h29zp3X+Lu84iOQRQDO0OMKWnuTnVDhHNOKeIv/7QQw/jkspd4YXvlO7OolSBEJI2FmSBeAaaa2SQzywOuAh6JPSFY27q9n+bLwEp3rwsxpqTVNbbQ3OoUDc5jxpihPHTd2ZxUOJAv3LWaP778NqAyGyKS3kJLEO7eAlwPPAFsBu5z901mttTMlganzQQ2mdkWom873dh+vZn9iehs7elmVm5mXwor1kSqg26kEUESGDtsIPctXciZE4azYssBQF1MIpLeQl1X2t2XA8vj9i2L+fklYGoX114dZmw9SdSNNGxgLnd/cQHfeWADG3bXMjAvu6/CExEJXagJoj9rH4iO70bKz8nmZ5+ah3tGVzsXkQwQ5hhEv9a+3sOILsYZzFSkT0TSmxJEF6obol1MXSUIEZF0pwTRhaqGCIPzcxiQq3EGEclMShBdqKqPqJSGiGQ0JYguVDdE1L0kIhlNCaILlarWKiIZTgmiC9UNEc2UFpGMpgSRQHsdJo1BiEgmU4JIoO5ICy1trjEIEcloShAJVDa0r/egMQgRyVxKEAnEF+oTEclEShAJtJfZ0BiEiGQyJYgEqoIuJr3mKiKZTAkigeqgBTF8UG4fRyIi0neUIBKoaogwZEAO+TmqwyQimUsJIoEqTZITEVGCSKSqvqnTSnIiIpko1ARhZovMbKuZbTezmxIcH25mD5lZmZmtNrPZyV4bJhXqExEJMUGYWTZwG7AYmAVcbWaz4k67GVjn7nOAa4Bb38W1oamsj1CkV1xFJMOF2YJYAGx39x3uHgHuBS6NO2cWsALA3bcAE81sdJLXhqKtzTl4WC0IEZEwE8Q4YFfMdnmwL9Z64AoAM1sATABKkryW4LprzazUzEorKiqOO+i6xmZa21xzIEQk44WZICzBPo/bvgUYbmbrgBuAV4GWJK+N7nS/3d3nu/v84uLi4wg3qlKzqEVEAMgJ8bPLgfEx2yXAntgT3L0OWAJgZgbsDP4U9HRtWNrrMKkFISKZLswWxCvAVDObZGZ5wFXAI7EnmFlhcAzgy8DKIGn0eG1YquqjZTY0BiEimS60FoS7t5jZ9cATQDZwp7tvMrOlwfFlwEzgd2bWCrwGfKm7a8OKNVZV0ILQW0wikunC7GLC3ZcDy+P2LYv5+SVgarLX9oaqjjpMShAiktk0kzpOdUMTQwfkkJut/2lEJLPpWzBOZUNEK8mJiKAEcZTqek2SExEBJYijVDU0aQ6EiAhKEEeJFupTF5OIiBJEjLY2p7pBhfpEREAJopOaI820uSbJiYiAEkQn1Q3RWdRaLEhERAmik45CfWpBiIgoQcTqKNSnMQgRESWIWCrUJyLyDiWIGO2F+kYUKEGIiChBxKiqj1BYkEuO6jCJiChBxIpOklPrQUQElCA6qWpookizqEVEACWITqpUqE9EpIMSRIzqhohecRURCShBBFrbnOrDEU2SExEJKEEEag5HcFeZDRGRdqEmCDNbZGZbzWy7md2U4PgwM/ubma03s01mtiTm2I1mtjHY//Uw44SYORBqQYiIACEmCDPLBm4DFgOzgKvNbFbcadcBr7n7XOAC4Kdmlmdms4GvAAuAucBHzGxqWLFCdIAaVGZDRKRdmC2IBcB2d9/h7hHgXuDSuHMcGGJmBgwGqoEWYCawyt0Pu3sL8CxweYixUtVeyVWvuYqIAOEmiHHArpjt8mBfrF8STQZ7gA3Aje7eBmwEzjOzkWZWAHwIGJ/oJmZ2rZmVmllpRUXFMQerQn0iIp2FmSAswT6P2/4gsA44CZgH/NLMhrr7ZuAnwFPA48B6oi2Loz/Q/XZ3n+/u84uLi4852Mr6CGYwXHWYRESAcBNEOZ1/6y8h2lKItQR40KO2AzuBGQDufoe7n+Hu5xHtetoWYqxUNzQxvCCP7KxEeU1EJPOEmSBeAaaa2SQzywOuAh6JO+dt4GIAMxsNTAd2BNujgr9PBq4A/hRirKrDJCISJyesD3b3FjO7HngCyAbudPdNZrY0OL4M+Bfgt2a2gWiX1LfdvTL4iAfMbCTQDFzn7gfDihWiXUxKECIi7wgtQQC4+3Jgedy+ZTE/7wEu6eLa94UZW7zqhgjTRg/uzVuKiJzQNJM6UFXfpBaEiEgMJQigpbWNmiPNmgMhIhJDCQI4eLg5qMOkFoSISDslCGImyakFISLSQQmC6PgDqFCfiEgsJQjeqeRapC4mEZEOShC808WkFoSIyDuUIIh2MWUZFKoOk4hIByUIol1MqsMkItKZEgTRxYL0iquISGdKEKhQn4hIIkoQQGVDk+ZAiIjEUYIg2oJQF5OISGcZnyDcnQunj+L0kwv7OhQRkRNKqOW++wMz4z8/Na+vwxAROeFkfAtCREQSU4IQEZGElCBERCShUBOEmS0ys61mtt3MbkpwfJiZ/c3M1pvZJjNbEnPsG8G+jWb2JzMbEGasIiLSWWgJwsyygduAxcAs4GozmxV32nXAa+4+F7gA+KmZ5ZnZOOCfgfnuPhvIBq4KK1YRETlamC2IBcB2d9/h7hHgXuDSuHMcGGJmBgwGqoGW4FgOMNDMcoACYE+IsYqISJwwE8Q4YFfMdnmwL9YvgZlEv/w3ADe6e5u77wb+A3gb2AvUuvuTiW5iZteaWamZlVZUVKT6GUREMlaYCSJRaVSP2/4gsA44CZgH/NLMhprZcKKtjUnBsUFm9tlEN3H32919vrvPLy4uTlXsIiIZL8yJcuXA+JjtEo7uJloC3OLuDmw3s53ADGACsNPdKwDM7EHgbOAP3d1wzZo1lWb21jHGWwRUHuO1/ZmeO7PouTNLMs89oasDYSaIV4CpZjYJ2E10kPnTcee8DVwMPGdmo4HpwA6irY+zzKwAOBKcU9rTDd39mJsQZlbq7vOP9fr+Ss+dWfTcmeV4nzu0BOHuLWZ2PfAE0beQ7nT3TWa2NDi+DPgX4LdmtoFoUvi2u1cClWZ2P7CW6KD1q8DtYcUqIiJHC7UWk7svB5bH7VsW8/Me4JIurv0+8P0w4xMRka5pJvU7MrWFoufOLHruzHJcz23R8WEREZHO1IIQEZGElCBERCShjE8QPRUUTCdmdqeZHTCzjTH7RpjZU2a2Lfh7eF/GmGpmNt7MnjazzUHxxxuD/en+3APMbHVMIcwfBvvT+rnbmVm2mb1qZo8G25ny3G+a2QYzW2dmpcG+Y372jE4QSRYUTCe/BRbF7bsJWOHuU4EVwXY6aQG+6e4zgbOA64L/xun+3E3ARUEhzHnAIjM7i/R/7nY3AptjtjPluQEudPd5MfMfjvnZMzpBkFxBwbTh7iuJFkSMdSlwd/Dz3cBlvRlT2Nx9r7uvDX4+RPRLYxzp/9zu7vXBZm7wx0nz5wYwsxLgw8BvYnan/XN345ifPdMTRDIFBdPdaHffC9EvU2BUH8cTGjObCJwOvEwGPHfQzbIOOAA85e4Z8dzAz4FvAW0x+zLhuSH6S8CTZrbGzK4N9h3zs4c6Ua4fSKagoKQBMxsMPAB83d3rohXm05u7twLzzKwQeMjMZvdxSKEzs48AB9x9jZld0Mfh9IVz3H2PmY0CnjKzLcfzYZnegkimoGC6229mYwGCvw/0cTwpZ2a5RJPDPe7+YLA77Z+7nbvXAM8QHX9K9+c+B/iYmb1JtMv4IjP7A+n/3EBHdQrc/QDwENFu9GN+9kxPEB0FBc0sj2hBwUf6OKbe9gjw+eDnzwMP92EsKRcsRnUHsNndfxZzKN2fuzhoOWBmA4H3A1tI8+d29++4e4m7TyT67/kf7v5Z0vy5AcxskJkNaf+ZaBmjjRzHs2f8TGoz+xDRPsv2goI/7tuIwmNmfyK6tGsRsJ9orau/AvcBJxOtrvsJd48fyO63zOxc4DmiC1K190nfTHQcIp2few7RAclsor8I3ufuPzKzkaTxc8cKupj+l7t/JBOe28wmE201QHT44I/u/uPjefaMTxAiIpJYpncxiYhIF5QgREQkISUIERFJSAlCREQSUoIQEZGElCBE+pCZXdBecVTkRKMEISIiCSlBiCTBzD4brK+wzsz+OyiEV29mPzWztWa2wsyKg3PnmdkqMyszs4fa6++b2Slm9vdgjYa1ZjYl+PjBZna/mW0xs3uC2d+Y2S1m9lrwOf/RR48uGUwJQqQHZjYT+BTRQmjzgFbgM8AgYK27nwE8S3RmOsDvgG+7+xyiM7jb998D3Bas0XA2sDfYfzrwdaJrkkwGzjGzEcDlwKnB5/xrmM8okogShEjPLgbOBF4JymdfTPSLvA34c3DOH4BzzWwYUOjuzwb77wbOC2rkjHP3hwDcvdHdDwfnrHb3cndvA9YBE4E6oBH4jZldAbSfK9JrlCBEembA3cEqXfPcfbq7/yDBed3VremuvnhTzM+tQI67txCtxPkA0QVeHn93IYscPyUIkZ6tAK4Mauy3r/E7gei/nyuDcz4NPO/utcBBM3tfsP9zwLPuXgeUm9llwWfkm1lBVzcM1q8Y5u7LiXY/zUv5U4n0INMXDBLpkbu/Zmb/h+hKXVlAM3Ad0ACcamZrgFqi4xQQLam8LEgAO4Alwf7PAf9tZj8KPuMT3dx2CPCwmQ0g2vr4RoofS6RHquYqcozMrN7dB/d1HCJhUReTiIgkpBaEiIgkpBaEiIgkpAQhIiIJKUGIiEhCShAiIpKQEoSIiCT0/wE9bwEFrp3MJwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "\n",
    "plt.plot(range(50),hist.history['val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
